{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e460eb9",
   "metadata": {},
   "source": [
    "## Apa itu Apache Spark\n",
    "Apache Spark adalah teknologi komputasi cluster, dirancang untuk melakukan komputasi dengan cepat. Ini dibuat diatas pada Hadoop MapReduce dan memperluas model MapReduce secara efisien untuk dapat melakukan lebih banyak jenis komputasi, yang mencakup queri interaktif dan pemrosesan realtime data.\n",
    "\n",
    "Apache Spark ditulis menggunakan bahasa pemrograman [Scala](https://scala-lang.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b8a17",
   "metadata": {},
   "source": [
    "## Apa itu PySpark\n",
    "Pyspark adalah koneksi antara Apache Spark dan Python. Ini adalah API Spark Python dan membantu kita terhubung dengan Resilient Distributed Datasets (RDDs) ke Apache Spark dan Python. PySpark adalah Python API untuk Spark yang dirilis oleh komunitas Apache Spark untuk mendukung Python dengan Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68b39c",
   "metadata": {},
   "source": [
    "## Apache Spark menggunakan Scala vs PySpark\n",
    "| Kriteria | PySpark | Scala |\n",
    "| :-- | :-- | :-- |\n",
    "| Performance Speed | Python relatif lebih lambat daripada Scala saat digunakan dengan Spark, tetapi programmer dapat melakukan lebih banyak hal dengan Python daripada dengan Scala karena Python menyediakan antarmuka yang lebih mudah | Spark ditulis dalam Scala, sehingga terintegrasi dengan baik dengan Scala. Ini lebih cepat dari Python |\n",
    "| Learning Curve | Python dikenal karena sintaksnya yang mudah dan merupakan bahasa tingkat tinggi yang lebih mudah dipelajari. Ini juga sangat produktif bahkan dengan sintaks yang sederhana | Scala memiliki syntax mirip dengan Java, setiap kali melakukan pekerjaan kita harus melakukan compile, install dependencies menggunakan sbt atau maven, dll |\n",
    "| Readability of Code | Readability, maintenance, dan familiarity dari code lebih baik di Python | Di Scala API, mudah untuk membuat perubahan internal karena Spark ditulis dalam Scala, kemungkinan versi Spark dengan Scala lebih update |\n",
    "| Complexity | Python dikenal sebagai bahasa pemrograman yang simple | Scala termasuk bahasa pemrograman yang cukup complex |\n",
    "| Machine Learning Libraries | Python lebih disukai untuk mengimplementasikan algoritma Machine Learning | Dukungan untuk Machine Learning dan Data Science tidak sebaik Python |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb592d2",
   "metadata": {},
   "source": [
    "## Instalations\n",
    "\n",
    "### MacOS\n",
    "- [Install Pyspark on Windows, Mac & Linux](https://www.datacamp.com/tutorial/installation-of-pyspark#mac-installation)\n",
    "- [How to Install and Use Homebrew](https://www.datacamp.com/tutorial/homebrew-install-use)\n",
    "- [Install Apache Spark on macOS](https://notadatascientist.com/install-spark-on-macos/)\n",
    "\n",
    "### Windows\n",
    "[Install Pyspark on Windows, Mac & Linux](https://www.datacamp.com/tutorial/installation-of-pyspark#windows-installation)\n",
    "\n",
    "### Docker\n",
    "[jupyter/pyspark-notebook - Docker Image](https://hub.docker.com/r/jupyter/pyspark-notebook/)\n",
    "\n",
    "### Conda\n",
    "[Install PySpark in Anaconda & Jupyter Notebook](https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/) - <b>Tested and Works</b>\n",
    "\n",
    "### Alternative\n",
    "[Apache Spark in Python with PySpark](https://www.datacamp.com/tutorial/apache-spark-python#How-to-Install-Spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230fce4",
   "metadata": {},
   "source": [
    "## Cluster Manager Types\n",
    "Saat menulis tutorial Spark dengan Python (PySpark), Spark mendukung cluster di bawah ini:\n",
    "\n",
    "- Standalone – cluster manager sederhana yang disertakan dengan Spark yang memudahkan pengaturan cluster.\n",
    "- Apache Mesos – Mesos adalah Cluster manager yang juga dapat menjalankan aplikasi Hadoop MapReduce dan PySpark.\n",
    "- Hadoop YARN – Resource manager di Hadoop 2. Ini adalah manager cluster yang sering digunakan.\n",
    "- Kubernetes – Open source system untuk mengotomatiskan deployment, scaling, dan pengelolaan aplikasi dalam container.\n",
    "\n",
    "local – yang sebenarnya bukan cluster manager tetapi karena kita akan menggunakan \"lokal\" untuk master() untuk menjalankan Spark di laptop/komputer kita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e51e27",
   "metadata": {},
   "source": [
    "## Membuat Spark Session\n",
    "\n",
    "SparkSession adalah titik masuk ke semua fungsionalitas di Spark, dan diperlukan jika kita ingin membangun kerangka data di PySpark. Jalankan baris kode berikut untuk menginisialisasi SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32195794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba83a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/29 11:57:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark_session = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Spark Intro')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c1423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ac55ee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "attachments": {
    "How-Apache-Spark-Context-is-created.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "46413656",
   "metadata": {},
   "source": [
    "## Membuat Spark Context\n",
    "SparkContext adalah klien dari lingkungan eksekusi Spark dan bertindak sebagai master aplikasi Spark. SparkContext menyiapkan layanan internal dan membuat koneksi ke lingkungan eksekusi Spark. \n",
    "\n",
    "Kita dapat membuat RDD, akumulator, dan variabel, mengakses layanan Spark dan menjalankan pekerjaan (hingga SparkContext berhenti) setelah pembuatan SparkContext. Hanya satu SparkContext yang dapat aktif per JVM. Kita harus menghentikan () SparkContext yang aktif sebelum membuat yang baru.\n",
    "\n",
    "![How-Apache-Spark-Context-is-created.png](attachment:How-Apache-Spark-Context-is-created.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742a35ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Intro>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark_session.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b392bd6",
   "metadata": {},
   "source": [
    "## RDD (Resilient Distributed Datasets)\n",
    "\n",
    "<img src=\"src/images/RDD.png\" width=\"500\">\n",
    "\n",
    "- Resilient: Ability to withstand failures \n",
    "- Distributed: Spanning across multiple machines \n",
    "- Datasets: Collection of partitioned data e.g, Arrays, Tables, Tuples etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887224df",
   "metadata": {},
   "outputs": [],
   "source": [
    "numRDD = sc.parallelize([1,2,3,4,5])\n",
    "helloRDD = sc.parallelize(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe05daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(helloRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f2c9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "print(helloRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0eae74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helloRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d8e6bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), (\"Marketing\",20), (\"Sales\",30), (\"IT\",40)]\n",
    "deptRDD = sc.parallelize(dept)\n",
    "dataColl=deptRDD.collect()\n",
    "for row in dataColl:\n",
    "    print(row[0] + \",\" +str(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34ad0a",
   "metadata": {},
   "source": [
    "### Understanding Partitioning in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa4699a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helloRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782e0d3",
   "metadata": {},
   "source": [
    "Secara default, Spark akan membagi data ke dalam 4 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f51ec10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD = sc.parallelize([1,2,3,4,5], 5)\n",
    "numRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf053e7",
   "metadata": {},
   "source": [
    "Kita dapat mengubah jumlah partition dengan menambahkan parameter sewaktu membuat RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfab26e",
   "metadata": {},
   "source": [
    "## PySpark Operations\n",
    "<img src=\"src/images/spark-opeations.png\" width=\"500\">\n",
    "\n",
    "- Transformations create new RDDS \n",
    "- Actions perform computation on the RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95306d7",
   "metadata": {},
   "source": [
    "## RDD Transformations\n",
    "\n",
    "- Transformations merupakan suatu Lazy evaluation (berarti eksekusi tidak akan dimulai sampai tindakan dipicu)\n",
    "\n",
    "<img src=\"src/images/rdd-transformation.png\" width=\"500\">\n",
    "\n",
    "- Basic RDD Transformations adalah map() , filter() , flatMap() , and union()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b0b7c",
   "metadata": {},
   "source": [
    "### map() Transformation\n",
    "transformasi map() menerapkan fungsi ke semua elemen di RDD\n",
    "\n",
    "<img src=\"src/images/map.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d4df12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([1,2,3,4]) \n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b0e69",
   "metadata": {},
   "source": [
    "### filter() Transformation\n",
    "Transformasi filter mengembalikan RDD baru dengan hanya elemen yang sesuai dengan kondisi tertentu\n",
    "\n",
    "<img src=\"src/images/filter.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d3bbecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([1,2,3,4]) \n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "RDD_filter.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ecef5",
   "metadata": {},
   "source": [
    "### flatMap() Transformation\n",
    "transformasi flatMap() mengembalikan beberapa nilai untuk setiap elemen dalam RDD asli\n",
    "\n",
    "<img src=\"src/images/flatmap.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "315e2d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = sc.parallelize([\"hello world\", \"how are you\"]) \n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "RDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed89829",
   "metadata": {},
   "source": [
    "### union() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69aeb7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_1 = sc.parallelize([1,2,3,4]) \n",
    "RDD_2 = sc.parallelize([5,6,7,8]) \n",
    "combinedRDD = RDD_1.union(RDD_2)\n",
    "combinedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad8d93",
   "metadata": {},
   "source": [
    "## RDD Actions \n",
    "- Operasi ini mengembalikan sebuah nilai setelah kita menjalankan komputasi pada RDD \n",
    "- Basic RDD Actions antara lain: collect() take(N) ,first() count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8843d2f",
   "metadata": {},
   "source": [
    "### collect() and take() Actions\n",
    "- collect() mengembalikan semua elemen dataset sebagai array \n",
    "- take(N) mengembalikan array dengan elemen N pertama dari dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2fb6739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b5ba00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501612b",
   "metadata": {},
   "source": [
    "### first() and count() Actions\n",
    "- first() mencetak elemen pertama dari RDD\n",
    "- count() mengembalikan jumlah elemen dalam RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "937f3787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_map.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f47166a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD_flatmap.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f89a7",
   "metadata": {},
   "source": [
    "### reduce() \n",
    "digunakan untuk menggabungkan elemen-elemen dari RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a26a2c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,3,4,6] \n",
    "RDD = sc.parallelize(x) \n",
    "RDD.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9b9cb",
   "metadata": {},
   "source": [
    "## Pair RDDs in PySpark\n",
    "- Kumpulan data sebenarnya biasanya merupakan pasangan key/value.\n",
    "- Setiap baris adalah key dan memetakan ke satu atau lebih value.\n",
    "- Pair RDD: Key-nya adalah identifier, dan value-nya adalah data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74994d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sam', 23), ('Mary', 34), ('Peter', 25)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)] \n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "pairRDD_tuple.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7311cfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sam', '23'), ('Mary', '34'), ('Peter', '25')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25'] \n",
    "regularRDD = sc.parallelize(my_list) \n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "pairRDD_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fb4ed",
   "metadata": {},
   "source": [
    "## Transformations on pair RDDs\n",
    "Semua transformasi sebelumnya yang telah dibahas dapat digunakan pada pasangan RDD. Bedanya adalah transformasi harus dapat melewati fungsi yang beroperasi pada pasangan key/value daripada pada elemen individu.\n",
    "\n",
    "Contoh Transformasi pada Pair RDD.\n",
    "- reduceByKey(func): Gabungkan nilai dengan key yang sama.\n",
    "- groupByKey(): Mengelompokkan nilai dengan key yang sama.\n",
    "- sortByKey(): Mengembalikan RDD yang diurutkan berdasarkan key.\n",
    "- join(): Join dengan dua pasangan RDD berdasarkan key-nya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfa5fa",
   "metadata": {},
   "source": [
    "### reduceByKey() transformation\n",
    "- transformasi reduceByKey() menggabungkan value dengan key yang sama.\n",
    "- Ini menjalankan operasi paralel untuk setiap key dalam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "748aac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neymar', 22), ('Ronaldo', 34), ('Messi', 47)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34), (\"Neymar\", 22), (\"Messi\", 24)]) \n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y) \n",
    "pairRDD_reducebykey.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7153d2e",
   "metadata": {},
   "source": [
    "### sortByKey() transformation\n",
    "- sortByKey() perintah operasi memasangkan RDD dengan key .\n",
    "- Ini mengembalikan RDD yang diurutkan berdasarkan key dalam urutan naik atau turun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd4ca187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0])) \n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11ab69",
   "metadata": {},
   "source": [
    "### groupByKey() transformation\n",
    "mengelompokkan semua nilai dengan key yang sama dalam pasangan RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f348b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US ['JFK', 'SFO']\n",
      "FR ['CDG']\n",
      "UK ['LHR']\n"
     ]
    }
   ],
   "source": [
    "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")] \n",
    "regularRDD = sc.parallelize(airports) \n",
    "pairRDD_group = regularRDD.groupByKey().collect() \n",
    "for cont, air in pairRDD_group: \n",
    "    print(cont, list(air))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf459d0",
   "metadata": {},
   "source": [
    "### join() transformation\n",
    "transformasi yang menggabungkan dua pasangan RDD berdasarkan key-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0beaafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ronaldo', (32, 80)), ('Neymar', (24, 120)), ('Messi', (34, 100))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)]) \n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9415a7",
   "metadata": {},
   "source": [
    "## Action Operations on pair RDDs\n",
    "Beberapa contoh actions pada Pair RDD antara lain:\n",
    "countByKey() collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26da447",
   "metadata": {},
   "source": [
    "### countByKey() action\n",
    "menghitung jumlah elemen untuk setiap key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b71c0cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2\n",
      "b 1\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)]) \n",
    "for key, val in rdd.countByKey().items(): \n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8be798",
   "metadata": {},
   "source": [
    "### collectAsMap() action\n",
    "mengembalikan pasangan key-value dalam RDD sebagai dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0de28f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0d433",
   "metadata": {},
   "source": [
    "## PySpark Dataframe\n",
    "- PySpark DataFrame adalah kumpulan data yang terdistribusi yang tidak dapat diubah (immutable).\n",
    "- PySpark SQL adalah library Spark untuk structured data. Ini dapat memberikan banyak informasi tentang struktur data dan komputasi. \n",
    "- Dirancang untuk memproses baik data terstruktur (misalnya database relasional) dan data semi-terstruktur (misalnya JSON)\n",
    "- Dataframe API tersedia di Python, R , Scala, dan Java.\n",
    "- DataFrames di PySpark mendukung query SQL ( SELECT * from table ) atau metode ekspresi ( df.select() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2506394",
   "metadata": {},
   "source": [
    "## Create a DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8996f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iphones_RDD = sc.parallelize([(\"XS\", 2018, 5.65, 2.79, 6.24), \n",
    "                              (\"XR\", 2018, 5.94, 2.98, 6.84), \n",
    "                              (\"X10\", 2017, 5.65, 2.79, 6.13), \n",
    "                              (\"8Plus\", 2017, 6.23, 3.07, 7.12) \n",
    "                             ])\n",
    "\n",
    "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\n",
    "\n",
    "iphones_df = spark_session.createDataFrame(iphones_RDD, schema=names) \n",
    "type(iphones_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6dff22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+-----+------+\n",
      "|Model|Year|Height|Width|Weight|\n",
      "+-----+----+------+-----+------+\n",
      "|   XS|2018|  5.65| 2.79|  6.24|\n",
      "|   XR|2018|  5.94| 2.98|  6.84|\n",
      "|  X10|2017|  5.65| 2.79|  6.13|\n",
      "|8Plus|2017|  6.23| 3.07|  7.12|\n",
      "+-----+----+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iphones_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e3eb7",
   "metadata": {},
   "source": [
    "## Create a DataFrame from reading file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c0a18",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "- Flight Status Prediction:\n",
    "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022?select=Combined_Flights_2021.csv\n",
    "\n",
    "- Dataset Medium dari Project 3 (\"dataset-medium.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e28e8eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anggapradikta/Documents/spark'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "400d501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = 'src/data/dataset-medium.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6aca7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "        .add(\"first_name\",StringType(), True) \\\n",
    "        .add(\"last_name\",StringType(),True) \\\n",
    "        .add(\"email\",StringType(),True) \\\n",
    "        .add(\"alamat\",StringType(),True) \\\n",
    "        .add(\"created_at\",StringType(),True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "baf18bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark_session.read.csv(file_csv, schema=schema, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a88b2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+--------------------+--------------------+\n",
      "|first_name|  last_name|               email|              alamat|          created_at|\n",
      "+----------+-----------+--------------------+--------------------+--------------------+\n",
      "|     Galih| Kuswandari|  iriyanti@yahoo.com|Jl. Raya Ujungber...|2018-07-08T13:31:...|\n",
      "|      Ikin|   Haryanti|   warta96@yahoo.com|Gang Asia Afrika ...|2018-07-27T07:19:...|\n",
      "|     Warta|Pudjiastuti|kemalfarida@perum...|Gg. Rajawali Timu...|2018-03-19T20:19:...|\n",
      "|    Rahmat|   Prasetyo|  tusada@hotmail.com|Lr. Sukajadi No. ...|2018-03-23T10:14:...|\n",
      "|     Cindy|    Yolanda|candrakantasantos...|Lr. Kiaracondong ...|2018-12-11T09:31:...|\n",
      "+----------+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7c7f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- alamat: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e91b6e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------------------+--------------------+--------------------+\n",
      "|summary|first_name| last_name|               email|              alamat|          created_at|\n",
      "+-------+----------+----------+--------------------+--------------------+--------------------+\n",
      "|  count|    300000|    300000|              300000|              300000|              300000|\n",
      "|   mean|      null|      null|                null|                null|                null|\n",
      "| stddev|      null|      null|                null|                null|                null|\n",
      "|    min|    Abyasa|Adriansyah|  aadriansyah@cv.edu|Gang Abdul Muis  ...|2018-01-01T00:02:...|\n",
      "|    max|     Zulfa|Zulkarnain|zzulkarnain@yahoo...|Lr. W.R. Supratma...|2018-12-24T23:59:...|\n",
      "+-------+----------+----------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 93:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_csv.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e299e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_csv_2 = 'src/data/Combined_Flights_2021.csv'\n",
    "df_csv_2 = spark_session.read.csv(file_csv_2, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77b4034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6311871"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "54864dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Cancelled: boolean (nullable = true)\n",
      " |-- Diverted: boolean (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Marketing_Airline_Network: string (nullable = true)\n",
      " |-- Operated_or_Branded_Code_Share_Partners: string (nullable = true)\n",
      " |-- DOT_ID_Marketing_Airline: integer (nullable = true)\n",
      " |-- IATA_Code_Marketing_Airline: string (nullable = true)\n",
      " |-- Flight_Number_Marketing_Airline: integer (nullable = true)\n",
      " |-- Operating_Airline: string (nullable = true)\n",
      " |-- DOT_ID_Operating_Airline: integer (nullable = true)\n",
      " |-- IATA_Code_Operating_Airline: string (nullable = true)\n",
      " |-- Tail_Number: string (nullable = true)\n",
      " |-- Flight_Number_Operating_Airline: integer (nullable = true)\n",
      " |-- OriginAirportID: integer (nullable = true)\n",
      " |-- OriginAirportSeqID: integer (nullable = true)\n",
      " |-- OriginCityMarketID: integer (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- OriginState: string (nullable = true)\n",
      " |-- OriginStateFips: integer (nullable = true)\n",
      " |-- OriginStateName: string (nullable = true)\n",
      " |-- OriginWac: integer (nullable = true)\n",
      " |-- DestAirportID: integer (nullable = true)\n",
      " |-- DestAirportSeqID: integer (nullable = true)\n",
      " |-- DestCityMarketID: integer (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DestState: string (nullable = true)\n",
      " |-- DestStateFips: integer (nullable = true)\n",
      " |-- DestStateName: string (nullable = true)\n",
      " |-- DestWac: integer (nullable = true)\n",
      " |-- DepDel15: double (nullable = true)\n",
      " |-- DepartureDelayGroups: double (nullable = true)\n",
      " |-- DepTimeBlk: string (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- WheelsOff: double (nullable = true)\n",
      " |-- WheelsOn: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDel15: double (nullable = true)\n",
      " |-- ArrivalDelayGroups: double (nullable = true)\n",
      " |-- ArrTimeBlk: string (nullable = true)\n",
      " |-- DistanceGroup: integer (nullable = true)\n",
      " |-- DivAirportLandings: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0c193",
   "metadata": {},
   "source": [
    "## Interacting with PySpark DataFrames\n",
    "- DataFrame operations: Transformations and Actions\n",
    "- DataFrame Transformations: select(), filter(), groupby(), orderby(), dropDuplicates() and withColumnRenamed()\n",
    "- DataFrame Actions : printSchema(), head(), show(), count(), columns and describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f10423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Airline|\n",
      "+--------------------+\n",
      "|SkyWest Airlines ...|\n",
      "|SkyWest Airlines ...|\n",
      "|SkyWest Airlines ...|\n",
      "|SkyWest Airlines ...|\n",
      "|SkyWest Airlines ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() and show()\n",
    "df_airline = df_csv_2.select('Airline')\n",
    "df_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f3adc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Airline|\n",
      "+--------------------+\n",
      "|GoJet Airlines, L...|\n",
      "|   Endeavor Air Inc.|\n",
      "|       Allegiant Air|\n",
      "|SkyWest Airlines ...|\n",
      "|         Horizon Air|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates\n",
    "df_airline = df_airline.dropDuplicates()\n",
    "df_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bb585dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Airline|Month|\n",
      "+--------------------+-----+\n",
      "|Air Wisconsin Air...|   12|\n",
      "|Air Wisconsin Air...|   12|\n",
      "|Air Wisconsin Air...|   12|\n",
      "|Air Wisconsin Air...|   12|\n",
      "|Air Wisconsin Air...|   12|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# filter() and show()\n",
    "df_month = df_csv_2.select('Airline', 'Month').filter(df_csv_2.Month > 10)\n",
    "df_month.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99b6f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|OriginAirportID|count|\n",
      "+---------------+-----+\n",
      "|          14570|20857|\n",
      "|          13832|  628|\n",
      "|          11630| 4678|\n",
      "|          11146| 3415|\n",
      "|          13795| 3162|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# groupby() and count()\n",
    "df_airportid = df_csv_2.groupby('OriginAirportID') \n",
    "df_airportid.count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f11924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|OriginAirportID|count|\n",
      "+---------------+-----+\n",
      "|          10135| 5360|\n",
      "|          10136| 2389|\n",
      "|          10140|17891|\n",
      "+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 80:=========================================>            (155 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# orderby()\n",
    "df_airportid.count().orderBy('OriginAirportID').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7ec5227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        airline_name|\n",
      "+--------------------+\n",
      "|GoJet Airlines, L...|\n",
      "|   Endeavor Air Inc.|\n",
      "|       Allegiant Air|\n",
      "|SkyWest Airlines ...|\n",
      "|         Horizon Air|\n",
      "|United Air Lines ...|\n",
      "|Air Wisconsin Air...|\n",
      "|         Comair Inc.|\n",
      "|Frontier Airlines...|\n",
      "|Southwest Airline...|\n",
      "|     JetBlue Airways|\n",
      "|Commutair Aka Cha...|\n",
      "|Empire Airlines Inc.|\n",
      "|           Envoy Air|\n",
      "|Capital Cargo Int...|\n",
      "|Hawaiian Airlines...|\n",
      "|Alaska Airlines Inc.|\n",
      "|Delta Air Lines Inc.|\n",
      "|  Mesa Airlines Inc.|\n",
      "|American Airlines...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed()\n",
    "df_airline = df_airline.withColumnRenamed('Airline', 'airline_name')\n",
    "df_airline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a2c8a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline_name']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airline.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226bac1",
   "metadata": {},
   "source": [
    "## Interacting with DataFrames using PySpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "760f165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_2.createOrReplaceTempView(\"flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ad663c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|FlightDate|             Airline|\n",
      "+----------+--------------------+\n",
      "|2021-03-06|SkyWest Airlines ...|\n",
      "|2021-03-21|Capital Cargo Int...|\n",
      "|2021-12-25|Capital Cargo Int...|\n",
      "|2021-12-29|Alaska Airlines Inc.|\n",
      "|2021-12-17|     JetBlue Airways|\n",
      "|2021-12-05|         Comair Inc.|\n",
      "|2021-12-13|SkyWest Airlines ...|\n",
      "|2021-12-26|         Horizon Air|\n",
      "|2021-04-14|Delta Air Lines Inc.|\n",
      "|2021-04-16|Air Wisconsin Air...|\n",
      "|2021-04-10|GoJet Airlines, L...|\n",
      "|2021-04-11|    Spirit Air Lines|\n",
      "|2021-11-12|American Airlines...|\n",
      "|2021-11-24|American Airlines...|\n",
      "|2021-11-18|Capital Cargo Int...|\n",
      "|2021-11-06|    Spirit Air Lines|\n",
      "|2021-09-06|Delta Air Lines Inc.|\n",
      "|2021-09-13|Frontier Airlines...|\n",
      "|2021-09-19|       Allegiant Air|\n",
      "|2021-09-30|           Envoy Air|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = spark_session.sql(\"SELECT DISTINCT FlightDate, Airline FROM flight\") \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08c7267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d00784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
